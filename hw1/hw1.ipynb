{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 2 &mdash; Détection et mise en correspondance de primitives\n",
    "\n",
    "Les cellules de code dans ce blocs-note vous permettent de tester votre implémentation des différentes tâches de ce devoir. Si vos tests échouent, vous devez vérifier pourquoi \n",
    "ils ont échoué. En particulier, faites attention aux tolérances/seuils utilisés dans ce bloc-notes. Il est possible que votre réponse soit correcte, mais soit à peine en dehors de la plage de tolérance.\n",
    "\n",
    "## Initialisation du bloc-notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%%capture` not found.\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "# Exécutez cette cellule dans l'environement Colab uniquement - Sinon, ignorez.\n",
    "#\n",
    "%%capture\n",
    "!wget https://benhadid.github.io/m1vpo/static_files/assignments/hw1.zip\n",
    "!unzip hw1.zip\n",
    "!mv hw1/* .\n",
    "!rm -rf hw1\n",
    "!rm -rf hw1.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Numpy est le paquetage principal utilisé pour le calcul scientifique dans Python. \n",
    "import numpy as np\n",
    "\n",
    "# cv2 est le paquetage OpenCV pour Python - il est utilisé pour implémenter/utiliser \n",
    "# des fonctionalités en traintement d'image et/ou en vision par ordinateur\n",
    "import cv2\n",
    "\n",
    "# Ce module fournit une interface standard pour extraire, formater et imprimer \n",
    "# les traces de pile des programmes Python\n",
    "import traceback\n",
    "\n",
    "# PIL (Python Image Library) est un paquetage utilisé pour manipuler les images sous Python \n",
    "from PIL import Image\n",
    "\n",
    "# pyplot fournit les fonctions de dessin de plots et d'images \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Chargement du fichier de l'étudiant - c.-à-d. le fichier features.py\n",
    "import features\n",
    "\n",
    "# Quelques instructions supplémentaires pour que le notebook recharge les modules externes en python;\n",
    "# voir http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Sauvegarde et chargement de points cv2\n",
    "def pickle_cv2(arr):\n",
    "    index = []\n",
    "    for point in arr:\n",
    "        temp = (point.pt, point.size, point.angle, point.response, point.octave, point.class_id)\n",
    "        index.append(temp)\n",
    "    return np.array(index, dtype=object)\n",
    "\n",
    "def unpickle_cv2(arr):\n",
    "    index = []\n",
    "    for point in arr:\n",
    "        temp = cv2.KeyPoint(x=point[0][0],\n",
    "                            y=point[0][1],\n",
    "                            size=point[1], \n",
    "                            angle=point[2], \n",
    "                            response=point[3], \n",
    "                            octave=point[4], \n",
    "                            class_id=point[5])\n",
    "        index.append(temp)\n",
    "    return np.array(index)\n",
    "\n",
    "# Fonctions pour tester l'exactitude de deux tableaux élément-par-élément\n",
    "def compare_array(arr1, arr2):    \n",
    "    return np.allclose(arr1,arr2,rtol=1e-3,atol=1e-5)\n",
    "\n",
    "# Fonction pour comparer deux points cv2\n",
    "def compare_cv2_points(pnt1, pnt2):\n",
    "    if not np.isclose(pnt1.pt[0],pnt2.pt[0],rtol=1e-3,atol=1e-5): return False\n",
    "    if not np.isclose(pnt1.pt[1],pnt2.pt[1],rtol=1e-3,atol=1e-5): return False\n",
    "    if not np.isclose(pnt1.angle,pnt2.angle,rtol=1e-3,atol=1e-5): return False\n",
    "    if not np.isclose(pnt1.response,pnt2.response,rtol=1e-3,atol=1e-5): return False\n",
    "    return True\n",
    "\n",
    "# fonction appelée pour tester les différentes tâches à implémenter\n",
    "def try_this(todo, run, truth, compare, *args, **kargs):\n",
    "    '''\n",
    "    Exécute une fonction, teste le résultat avec 'compare', et affiche un message \n",
    "    d'erreur si ça ne fonctionne pas.\n",
    "    @arg todo (int or str): Le numéro du TODO \n",
    "    @arg run (func): La fonction à exécuter\n",
    "    @arg truth (any): Le résultat correct  de la fonction testée\n",
    "    @arg compare (func->bool): retourne un booléen de la comparaison de la sortie de la fonction `run` avec le résultat correct\n",
    "    @arg *args (any): Arguments supplémentaires éventuels à passer à la fonction `run`\n",
    "    @arg **kargs (any): Arguments supplémentaires éventuels à passer à la fonction `compare`\n",
    "\n",
    "    @return (int): Le nombre de tests qui ont échoué\n",
    "    '''\n",
    "    print('Starting test for TODO {}'.format(todo))\n",
    "    failed = 0\n",
    "    try:\n",
    "        output = run(*args)\n",
    "        print(\"ok\")\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "        print(\"TODO {} threw an exception, see exception below\\n\".format(todo))\n",
    "        return\n",
    "    if type(output) is list or type(output) is tuple:\n",
    "        for i in range(len(output)):\n",
    "            if not compare(output[i], truth[i], **kargs):\n",
    "                print(\"TODO {} doesn't pass test: {}\".format(todo, i))\n",
    "                failed+=1\n",
    "    else:\n",
    "        if not compare(output, truth, **kargs):\n",
    "            print(\"TODO {} doesn't pass test\".format(todo))\n",
    "            failed+=1\n",
    "    return failed\n",
    "\n",
    "# chargement de l'image du triangle pour les tests par défaut\n",
    "image = np.array(Image.open('resources/triangle1.jpg'))\n",
    "grayImage = cv2.cvtColor(image.astype(np.float32)/255.0, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "'''\n",
    "Chargement des tableaux numpy contenant les résultats de triangle1.jpg.\n",
    "\n",
    "Ces tableaux sont accessibles à l'aide de loaded['<lettre>']. \n",
    "Par exemple, la réponse correcte pour le test 2 est 'c', donc pour voir \n",
    "la réponse correcte pour le test 2 vous pouvez inspecter loaded['c'].\n",
    "Remarque importante : NumPy n'affiche pas l'ensemble du tableau s'il est \n",
    "très grand --- vous devez afficher des parties plus petites (e.g., \n",
    "print( repr(loaded['c'][0]) ) ).\n",
    "'''\n",
    "loaded = np.load('./resources/arrays.npz', allow_pickle=True)\n",
    "print()\n",
    "d = unpickle_cv2(loaded['d_proc'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Description\n",
    "\n",
    "Le but de la détection et de la mise en correspondance de primitives est d'établir le lien entre un point dans une image *X* et son correspondant dans une autre image *Y*. Ces correspondances peuvent ensuite être utilisées pour assembler plusieurs images en un panorama.\n",
    "\n",
    "Dans ce devoir, vous allez écrire du code pour détecter les primitives discriminantes (c.-à-d. qui sont raisonnablement invariantes à la translation, la rotation et le changement dans l'éclairage) dans une image et trouver les meilleures primitives correspondantes dans une autre image. Le devoir comprend trois parties : \n",
    " - détection des primitives, \n",
    " - description des primitives \n",
    " - et mise en correspondance des primitives. \n",
    "\n",
    "**Le fichier de démarrage fournit `features.py` contient une implémentation squelette du devoir. <span style='color:Red'> Le code que vous devez écrire et toutes les modifications requises sont indiqués dans ce fichier par les marqueurs '# TODO-BLOC-DEBUT' et '# TODO-BLOC-FIN'.</span>** \n",
    "\n",
    "Un certain nombre de classes et de méthodes vous sont déjà fournis. Il est primordial de comprendre le rôle de chaque classe et fonction avant d'essayer de modifier le code. Prenez donc le temps de consulter les fichiers sources et les commentaires fournis. \n",
    "\n",
    "Un exemple de détecteur / descripteur de primitive appelé \"[Oriented FAST and Rotated BRIEF (ORB)](https://medium.com/analytics-vidhya/introduction-to-orb-oriented-fast-and-rotated-brief-4220e8ec40cf)\" y est également fournie pour comparaison. Cette technique est très populaire en Vision par Ordinateur et se propose comme alternative aux algorithmes [SIFT](https://docs.opencv.org/3.4.9/da/df5/tutorial_py_sift_intro.html) et [SURF](https://docs.opencv.org/3.4.9/df/dd2/tutorial_py_surf_intro.html).\n",
    "\n",
    "Un ensemble d'images de référence est également fournit pour tester les performances de votre implémentation en fonction de différents types de variation contrôlée (c'est-à-dire, rotation, redimensionnement, éclairage, perspective, flou). Pour chacune de ces images, la transformation correcte est connue et pouvons donc mesurer la précision de chacune de vos détection et/ou mise en correspondance de primitives. Gardez à l'esprit que votre code sera évalué numériquement, pas visuellement.\n",
    "\n",
    "Vous devriez également prendre vos propres images pour voir à quel point votre approche fonctionne sur des ensembles de données plus intéressants.\n",
    "\n",
    "## 2. Visualisation\n",
    "\n",
    "Pour vous aider à visualiser vos résultats et déboguer votre implémentation, une interface graphique est fournie (fichier **`featuresUI.py`**). Cette interface utilisateur permet de visualiser les détections de points-clés et les résultats de correspondance des primitives. Les fonctions de détection de points-clés et les méthodes de mise en correspondance sont appelées à partir de cette interface. N'hésitez pas à étendre les fonctionnalités de cette interface, mais n'oubliez pas que seul le code dans **`features.py`** sera noté.\n",
    "\n",
    "L'exécution de l'interface **`featuresUI.py`** présente  les choix suivants :\n",
    "\n",
    "- **Détection des points-clés** : <br>\n",
    "  Vous pouvez charger une image et calculer les points d'intérêt avec leurs orientations.\n",
    "\n",
    "\n",
    "- **Mise en correspondance des primitives** : <br>\n",
    "  Ici, vous pouvez charger deux images et afficher les meilleures correspondances calculées à l'aide des algorithmes spécifiés.\n",
    "\n",
    "\n",
    "- **Analyse** : <br>\n",
    "  Après avoir spécifié le chemin d'accès au répertoire contenant l'ensemble des données, le programme exécutera les algorithmes spécifiés sur toutes les images et calculera les courbes ROC associées.\n",
    "\n",
    "## 3. Règles de codage\n",
    "\n",
    "- Vous pouvez utiliser les fonctions NumPy, SciPy et OpenCV pour implémenter des opérations mathématiques, de filtrage et/ou de transformation. Mais, vous ne devais en aucun cas utiliser les fonctions OpenCV qui implémentent la détection de points-clés ou la mise en correspondance de primitives - ça sera un zéro pour tout le devoir si vous le faites !\n",
    "\n",
    "- Lorsque vous utilisez l'opérateur de Sobel ou le filtre gaussien, vous devez utiliser le mode « réflexion » pour avoir un gradient nul sur les bords.\n",
    "\n",
    "- Voici une liste de fonctions potentiellement utiles (vous n'êtes pas obligé de les utiliser) :\n",
    "\n",
    "      - scipy.ndimage.sobel\n",
    "      - scipy.ndimage.gaussian_filter\n",
    "      - scipy.ndimage.filters.convolve\n",
    "      - scipy.ndimage.filters.maximum_filter\n",
    "      - scipy.spatial.distance.cdist\n",
    "      - cv2.warpAffine\n",
    "      - np.max, np.min, np.std, np.mean, np.argmin, np.argpartition\n",
    "      - np.degrees, np.radians, np.arctan2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Tâches principales\n",
    "\n",
    "### 4.1. Détection de points-clés ( 35 points )\n",
    "\n",
    "Dans cette étape, vous identifierez les points-clés dans l'image à l'aide de l'opérateur d'Harris pour la détection de coins. Ainsi, pour chaque point de l'image, vous devez réaliser les étapes suivantes (voir les slides du cours pour plus de détails) : \n",
    "\n",
    "1. Définissez une fenêtre de pixels $W$ autour de ce point.\n",
    "\n",
    "\n",
    "2. Calculez la matrice $\\mathbf{H}$ pour la fenêtre $W$, définie comme\n",
    "\n",
    "    $$\n",
    "    \\begin{align*}\n",
    "    H &= \\sum_{p \\in W} w_p \\nabla I_p (\\nabla I_p)^T  \\\\\n",
    "    &= \\sum_{p \\in W} w_p \\begin{bmatrix} \n",
    "                 {I_{x_p}}^2 & I_{x_p} I_{y_p} \\\\ \n",
    "                  I_{x_p} I_{y_p} & {I_{y_p}}^2\n",
    "               \\end{bmatrix} \\\\\n",
    "    &= \\sum_{p \\in W}     \\begin{bmatrix} \n",
    "                 w_p {I_{x_p}}^2 & w_p I_{x_p} I_{y_p} \\\\ \n",
    "                 w_p I_{x_p} I_{y_p} & w_p  {I_{y_p}}^2\n",
    "               \\end{bmatrix}  \\\\\n",
    "    &= \\begin{bmatrix} \n",
    "       \\sum_{p \\in W} w_p {I_{x_p}}^2 & \\sum_{p \\in W} w_p I_{x_p} I_{y_p} \\\\ \n",
    "       \\sum_{p \\in W} w_p I_{x_p} I_{y_p} & \\sum_{p \\in W} w_p  {I_{y_p}}^2\n",
    "    \\end{bmatrix}    \n",
    "    \\end{align*}    \n",
    "    $$\n",
    "\n",
    "    où la somme est sur tous les pixels *p* de la fenêtre $W$. \n",
    "    \n",
    "    $\\mathbf{H}$ est une matrice $2 \\times 2$. $I_{x_p}$ est la dérivée horizontale au point *p* de l'image, $I_{y_p}$ est la dérivée verticale en ce point. Vous devez utiliser l'opérateur de Sobel $3 \\times 3$ pour calculer les dérivées *x* et *y* (utilisez le mode \"réflexion\" pour extrapoler les pixels aux bords de l'image). Les poids $w_p$ doivent être circulairement symétriques (pour garder une invariance à la rotation) - utilisez un masque gaussien $5 \\times 5$ avec $\\sigma = 0.5$.<br>\n",
    "\n",
    "\n",
    "3. Utilisez ensuite $\\mathbf{H}$ pour calculer l'opérateur $f$. En ce sens, utilisez la définition ci-dessous de l'opérateur $f$ où $\\kappa$ est une constante déterminée empiriquement; utilisez la valeur $\\kappa = 0.1$.\n",
    " \n",
    "    $    \n",
    "     f = \\lambda_1 \\lambda_2 - \\kappa (\\lambda_1 + \\lambda_2)^2  = \\det(\\mathbf{H}) - \\kappa\\,\\text{trace}^2(\\mathbf{H})     \n",
    "    $\n",
    "\n",
    "4. Sélectionnez les points-clés qui correspondent aux maxima locaux de $f$. Utilisez une fenêtre $7 \\times 7$ pour vos calculs.\n",
    "\n",
    "\n",
    "5. Vous aurez également besoin de l'angle d'orientation à chaque pixel pour les questions suivantes du devoir. Utilisez l'angle du gradient comme une approximation. L'angle zéro pointe vers la droite et les angles positifs sont dans le sens de rotation des aiguilles d'une montre. **Remarque : Ne pas calculer l'orientation par la méthode des vecteurs propres**.\n",
    "\n",
    "\n",
    "####  Code à écrire\n",
    "\n",
    "La fonction **`detectKeypoints`** dans **` HarrisKeypointDetector`** est l'une des principales fonctions que vous devez compléter avec les fonctions auxilières **`computeHarrisValues`** (calcule le score et l'orientation de Harris pour chaque pixel de l'image) et **`computeLocalMaxima`** (calcule un tableau de booléens qui indique pour chaque pixel s'il est un maximum local ou non). L'ensemble de ces fonctions implémente le détecteur de coins de Harris. \n",
    "\n",
    "Les fonctions suivantes vous seront peut-être utiles pour implémenter cette tâche :\n",
    "\n",
    "- `scipy.ndimage.sobel`: Filtre l'image en entrée avec le filtre Sobel.\n",
    "- `scipy.ndimage.gaussian_filter`: Filtre l'image en entrée avec un filtre gaussien.\n",
    "- `scipy.ndimage.filters.maximum_filter`: Filtre l'image en entrée avec un filtre maximum.\n",
    "- `scipy.ndimage.filters.convolve`: Filtre l'image en entrée avec le noyau spécifié en paramètre.\n",
    "\n",
    "Pour plus de détails sur ces fonctions, veuillez consulter la [documentation en ligne](http://docs.scipy.org/doc/scipy/reference/ndimage.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test for TODO 1\n",
      "ok\n",
      "Starting test for TODO 2\n",
      "ok\n",
      "Starting test for TODO 3\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Nous allons tester ici votre implémentation des TODOs 1, 2 et 3 dans features.py\n",
    "sur l'exemple d'image triangles1.jpg - N'oubliez pas d'exécuter la cellule précédente\n",
    "de code Python pour initialiser l’environnement de test.\n",
    "'''\n",
    "\n",
    "# construction de l'objet HKD pour la détection des points-clés\n",
    "HKD = features.HarrisKeypointDetector()\n",
    "# appel de la fonction de test pour le TO-DO n°1\n",
    "try_this(1, HKD.computeHarrisValues, [loaded['a'],loaded['b']], compare_array, grayImage)\n",
    "\n",
    "# patch pour HKD afin que les futurs tests n'échouent pas si le dernier test a échoué\n",
    "class HKD2(features.HarrisKeypointDetector):\n",
    "  def computeHarrisValues(self,image):\n",
    "    return loaded['a'],loaded['b']\n",
    "HKD=HKD2()\n",
    "# appel de la fonction de test pour le TO-DO n°2\n",
    "try_this(2, HKD.computeLocalMaxima, loaded['c'], compare_array, loaded['a'])\n",
    "\n",
    "# patch pour HKD afin que les futurs tests n'échouent pas si le dernier test a échoué\n",
    "class HKD3(HKD2):\n",
    "  def computeLocalMaxima(self,image):\n",
    "    return loaded['c']\n",
    "HKD=HKD3()\n",
    "# appel de la fonction de test pour le TO-DO n°3\n",
    "try_this(3, HKD.detectKeypoints, d, compare_cv2_points, image)\n",
    "\n",
    "image2 = cv2.imread(\"resources/triangle2.jpg\")\n",
    "image1 = cv2.imread(\"resources/triangle1.jpg\")\n",
    "\n",
    "key_image2 = HKD.detectKeypoints(image2)\n",
    "key_image1 = HKD.detectKeypoints(image1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of features failed: Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Abdel Alim\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"c:\\Users\\Abdel Alim\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 480, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"c:\\Users\\Abdel Alim\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 377, in update_generic\n",
      "    update(a, b)\n",
      "  File \"c:\\Users\\Abdel Alim\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 345, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"c:\\Users\\Abdel Alim\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 302, in update_instances\n",
      "    if type(ref) is old:\n",
      "KeyboardInterrupt\n",
      "]\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3285531909.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [18]\u001b[1;36m\u001b[0m\n\u001b[1;33m    Une fois les points d'intérêt identifiés, l'étape suivante consiste à trouver un *descripteur* pour chaque point-clé détecté. Ce descripteur sera la représentation que vous utiliserez pour comparer des primitives dans différentes images et voir si elles correspondent. Vous implémenterez deux descripteurs pour ce devoir :\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### 4.2. Descripteur de primitive  ( 35 points )\n",
    "\n",
    "Une fois les points d'intérêt identifiés, l'étape suivante consiste à trouver un *descripteur* pour chaque point-clé détecté. Ce descripteur sera la représentation que vous utiliserez pour comparer des primitives dans différentes images et voir si elles correspondent. Vous implémenterez deux descripteurs pour ce devoir : \n",
    "\n",
    "- En premier, vous implémenterez un descripteur simple conçu des valeurs d'intensité de pixel dans un voisinage $5 \\times 5$ du point-clé. Cela devrait être facile à mettre en oeuvre et devrait fonctionner correctement quand les images comparées sont liées par une simple translation.\n",
    "\n",
    "\n",
    "- Deuxièmement, vous implémenterez une version simplifiée du descripteur MOPS. En ce sens, implémentez la matrice qui vous permettra de transformer une fenêtre $40 \\times 40$ en un patch de $8 \\times 8$ pixels après l'avoir pivotée d'un angle $\\theta$ autour du point-clé. La nouvelle orientation du point-clé devra être vers la droite. \n",
    "\n",
    "  Vous devez également normaliser les intensités du patch pour avoir une moyenne nulle et une variance égale à 1. Si la variance du patch avant normalisation est très proche de zéro (inférieure à $10^{-10}$ en valeur absolue), renvoyez simplement un descripteur entièrement nul pour éviter une erreur de division par zéro.\n",
    "  \n",
    "  Utilisez `cv2.warpAffine` de OpenCV pour effectuer la transformation. La fonction `warpAffine` requiert une matrice $2 \\times 3$ en entrée. La façon la plus simple de générer cette matrice est de combiner plusieurs transformations basiques. Une séquence de translation $\\mathbf{T_1}$, rotation $\\mathbf{R}$, redimensionnement $\\mathbf{S}$ et translation $\\mathbf{T_2}$ produira la matrice escompté. Notez que les transformations sont combinées de droite à gauche, de sorte que la matrice de transformation est le produit matriciel $\\mathbf{T_2\\;S\\;R\\;T_1}$. Les figures ci-dessous illustrent la séquence.\n",
    "  \n",
    "\n",
    "![](warpAffine.png)\n",
    "\n",
    "\n",
    "#### Code à écrire\n",
    "\n",
    "Vous devez implémenter deux descripteurs de primitives dans les classes **`SimpleFeatureDescriptor`** et **` MOPSFeaturesDescriptor`**. La méthode **`describeFeatures`** de ces classes prend en entrée les informations de position et d'orientation des points-clés déjà calculés (par exemple par le détecteur Harris) et calcule les descripteurs pour ces points-clés. Ces descripteurs sont ensuite stockés dans un tableau Numpy bidimensionnel. Le nombre de lignes de ce tableau est égale au nombre de point-clés détectés. Le nombre de colonnes du tableau est égale à la dimension du descripteur retourné (par exemple, 25 pour le descripteur simple de primitives $5 \\times 5$).\n",
    "\n",
    "Pour l'implémentation de MOPS, vous devez créer une matrice pour transformer une fenêtre $40 \\times 40$ en un patch de $8 \\times 8$ pixels après l'avoir pivotée d'un angle $\\theta$ autour du point-clé (comme décrit ci-dessus). \n",
    "\n",
    "Il est recommandé de consulter la [documentation opencv](https://docs.opencv.org/3.4.9/d4/d61/tutorial_warp_affine.html) sur **`cv2.warpAffine`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting test for TODO 4\n",
      "ok\n",
      "Starting test for TODO 5 and/or 6\n",
      "ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Nous allons tester ici votre implémentation des TODOs 4, 5 et 6 dans features.py\n",
    "sur l'exemple d'image triangles1.jpg\n",
    "'''\n",
    "\n",
    "# construction de l'objet SFD pour un descripteur simple d'intensités\n",
    "SFD = features.SimpleFeatureDescriptor()\n",
    "\n",
    "# appel de la fonction de test pour le TO-DO n°4\n",
    "try_this(4, SFD.describeFeatures, loaded['e'], compare_array, image, d)\n",
    "\n",
    "# construction de l'objet MFD pour un descripteur MOPS\n",
    "MFD = features.MOPSFeatureDescriptor()\n",
    "descripteur_image2 = MFD.describeFeatures(image2, key_image2)\n",
    "descripteur_image1 = MFD.describeFeatures(image1, key_image1)\n",
    "\n",
    "# appel de la fonction de test pour le TO-DO n°5 et/ou n°6\n",
    "try_this('5 and/or 6', MFD.describeFeatures, loaded['f'], compare_array, image, d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Mise en correspondance de primitives ( 30 points )\n",
    "\n",
    "Une fois les points-clés détectés et leurs descripteurs calculés pour un ensemble d'images, l'étape suivante consiste à établir les correspondances entre les différents points-clés identifiés dans les images (c.-à-d., étant donné une primitive dans une image *X*, trouver la meilleure primitive correspondante dans une autre image *Y*).\n",
    "\n",
    "\n",
    "L'approche la plus simple est la suivante : Pour chaque descripteur d'une image X, le comparer aux descripteurs d'une image *Y* en calculant une *distance* scalaire. La meilleure correspondance est la paire de primitives qui produit la plus petite distance. Vous implémenterez deux fonctions de distance :\n",
    "\n",
    "1. Somme des moindres carrés (SMC): il s'agit de la distance euclidienne au carré entre les deux descripteurs.\n",
    "\n",
    "\n",
    "2. Ratio de distances : le rapport des distances SMC associées aux deux meilleurs descripteurs identifiés (c.-à-d. le ratio est égale à la distance SMC du descripteur le plus proche divisée par la distance SMC du second descripteur le plus proche).\n",
    "\n",
    "#### Code à écrire\n",
    "\n",
    "Dans cette tache, vous allez implémenter une fonction pour la mise en correspondance de primitives. Vous implémenterez la fonction **`matchFeatures`** de **` SSDFeatureMatcher`** et de **`RatioFeatureMatcher`**. Ces fonctions renvoient une liste d'objets [cv2.DMatch](https://docs.opencv.org/3.4.9/d4/de0/classcv_1_1DMatch.html). \n",
    "\n",
    "Vous devez initialiser l'attribut `queryIdx` de `cv2.DMatch` à l'index de la primitive dans la première image, l'attribut` trainIdx` à l'index de la primitive dans la deuxième image et l'attribut `distance` à la distance (ou le ratio, selon le cas) entre les deux primitives. \n",
    "\n",
    "Les fonctions `scipy.spatial.distance.cdist` et` numpy.argmin` pourrait vous être utiles lors de l'implémentation de cette tache.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distance :  2.777191162109375\n",
      "indice keypoint de l'image 1 :  0\n",
      "indice keypoint de l'image 2 :  0\n",
      "\n",
      "distance :  0.5728821158409119\n",
      "indice keypoint de l'image 1 :  1\n",
      "indice keypoint de l'image 2 :  1\n",
      "\n",
      "distance :  1.7710464000701904\n",
      "indice keypoint de l'image 1 :  2\n",
      "indice keypoint de l'image 2 :  2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "En vous inspirant des tests précédents, vous pouvez implémenter ici vos tests\n",
    "pour les TODOs 7 et 8 dans features.py\n",
    "'''\n",
    "image2 = cv2.imread(\"resources/triangle1.jpg\")\n",
    "image1 = cv2.imread(\"resources/triangle2.jpg\")\n",
    "\n",
    "HKD8 = features.HarrisKeypointDetector()\n",
    "\n",
    "key_image2 = HKD8.detectKeypoints(image2)\n",
    "key_image1 = HKD8.detectKeypoints(image1)\n",
    "\n",
    "MFD = features.MOPSFeatureDescriptor()\n",
    "descripteur_image2 = MFD.describeFeatures(image2, key_image2)\n",
    "descripteur_image1 = MFD.describeFeatures(image1, key_image1)\n",
    "\n",
    "SSDFM = features.SSDFeatureMatcher()\n",
    "matches = SSDFM.matchFeatures(descripteur_image1, descripteur_image2)\n",
    "for f in matches:\n",
    "    print(\"distance : \",f.distance)\n",
    "    print(\"indice keypoint de l'image 1 : \",f.queryIdx)\n",
    "    print(\"indice keypoint de l'image 2 : \",f.trainIdx)\n",
    "    print()\n",
    "\n",
    "\n",
    "# TODO 7: tester la fonction de mise en correspondance 'matchFeatures' de la classe 'SSDFeatureMatcher' \n",
    "# TODO 8: tester la fonction de mise en correspondance 'matchFeatures' de la classe 'RatioFeatureMatcher'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//calculate ratio value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Tâche bonus (20 points)\n",
    "\n",
    "Des points supplémentaires sont accordés pour les solutions qui améliorent, d'au moins 15%, l'**A**ire **S**ous la **C**ourbe (ASC) moyenne pour le test utilisant le \"ratio de distances\". Une amélioration de 15% est interprétée comme une réduction de 15% de la valeur $(1 - ASC)$. C'est-à-dire la zone au-dessus de la courbe. Voici une suggestion (vous êtes également encouragés à proposer vos propres idées !).\n",
    "\n",
    " - Implémentez la méthode **`selectKeypointsANMS`** de la classe **`KeypointDetector`**. Cette méthode devrait réaliser la technique de suppression non maximale adaptative discutée dans l'[article MOPS](https://drive.google.com/open?id=1vly0rexasm-kM_lWEulJ-PiBfCDm8cdO)\n",
    "  \n",
    "Vous pouvez mesurer votre ASC moyenne en exécutant le benchmark de l'interface utilisateur **`featuresUI.py`** sur les cinq ensembles de données (bikes, graf, leuven, wall, yosemite). Nous évaluerons spécifiquement les performances de votre implémentation contre l'ensemble de données 'yosemite'. Les implémentations qui n'améliorent pas l'ASC moyenne d'au moins 15% ne seront pas notées.\n",
    "\n",
    "Le bonus sera accordé en fonction du mérite de votre amélioration et de **sa justification**. Les modifications hyperparamétriques simples recevront moins de points supplémentaires que les améliorations significatives de l'algorithme.\n",
    "\n",
    "Décrivez votre implémentation dans un fichier nommé **`Lisez-moi_adaptative.txt`** qui décrit vos modifications. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Bonus : dans features.py non implémenté",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Abdel Alim\\Documents\\zarty\\hw1\\hw1\\hw1.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abdel%20Alim/Documents/zarty/hw1/hw1/hw1.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m ref_keypoints \u001b[39m=\u001b[39m unpickle_cv2(loaded[\u001b[39m'\u001b[39m\u001b[39myosemite_proc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Abdel%20Alim/Documents/zarty/hw1/hw1/hw1.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ref_anms_keyoints \u001b[39m=\u001b[39m unpickle_cv2(loaded[\u001b[39m'\u001b[39m\u001b[39manms_proc\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Abdel%20Alim/Documents/zarty/hw1/hw1/hw1.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m student_anms_keypoints \u001b[39m=\u001b[39m HKD\u001b[39m.\u001b[39;49mselectKeypointsANMS(ref_keypoints,\u001b[39m100\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abdel%20Alim/Documents/zarty/hw1/hw1/hw1.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m Image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(\u001b[39m'\u001b[39m\u001b[39mresources/yosemite/yosemite1.jpg\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Abdel%20Alim/Documents/zarty/hw1/hw1/hw1.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m grayImage   \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(Image, cv2\u001b[39m.\u001b[39mCOLOR_BGR2GRAY)\n",
      "File \u001b[1;32mc:\\Users\\Abdel Alim\\Documents\\zarty\\hw1\\hw1\\features.py:55\u001b[0m, in \u001b[0;36mKeypointDetector.selectKeypointsANMS\u001b[1;34m(self, keypoints, maxNbrPoints)\u001b[0m\n\u001b[0;32m     50\u001b[0m features \u001b[39m=\u001b[39m []\n\u001b[0;32m     52\u001b[0m \u001b[39m# TODO Bonus : Implémentez ici la méthode de suppression non-maximale \u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m# adaptative pour la sélection des points-clés les plus pertinents\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \u001b[39m# TODO-BLOC-DEBUT\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mBonus : dans features.py non implémenté\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     56\u001b[0m \u001b[39m# TODO-BLOC-FIN\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[39mreturn\u001b[39;00m features\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Bonus : dans features.py non implémenté"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Nous allons tester ici votre implémentation de la tâche  bonus dans features.py\n",
    "sur l'exemple d'image yosemite1.jpg\n",
    "'''\n",
    "\n",
    "ref_keypoints = unpickle_cv2(loaded['yosemite_proc'])\n",
    "ref_anms_keyoints = unpickle_cv2(loaded['anms_proc'])\n",
    "student_anms_keypoints = HKD.selectKeypointsANMS(ref_keypoints,100)\n",
    "\n",
    "Image = cv2.imread('resources/yosemite/yosemite1.jpg')\n",
    "grayImage   = cv2.cvtColor(Image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# la selection ANMS réalisée par votre code sera affichée en bleu \n",
    "student_image = cv2.drawKeypoints(grayImage, student_anms_keypoints, 0, (0, 0, 255),\n",
    "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# la selection ANMS corrècte est affichée en rouge \n",
    "ref_image = cv2.drawKeypoints(grayImage, ref_anms_keyoints, 0, (255, 0, 0),\n",
    "                                 flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "# displaying the image with ref and anms keypoints as the\n",
    "# output on the screen\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9,4))\n",
    "\n",
    "axes[0].imshow(student_image, aspect='auto')\n",
    "axes[0].set_title(f'ANMS (étudiant)', fontsize=12)\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(ref_image, aspect='auto')\n",
    "axes[1].set_title(f'ANMS (référence)', fontsize=12)\n",
    "axes[1].axis('off')\n",
    "\n",
    "fig.tight_layout() # used to adjust padding between subplots "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Livrables\n",
    "    \n",
    "### 6.1. Le code (à remettre sur [benhadid.ddns.net](https://benhadid.ddns.net/course/M1_VPO/hw1))\n",
    "\n",
    "Un fichier zip contenant le fichier **`features.py`** et éventuellement un fichier **`Lisez-moi_adaptative.txt`** si vous avez implémenté la tache bonus.\n",
    "\n",
    "**Le code sera remis <del>en classe pendant votre séance de TP </del> au serveur INGInious - <span style='color:Red'> aucun document ou code ne sera accepté si envoyé par mail ou présenté sur clé USB</span>**.\n",
    "\n",
    "<del>\n",
    "\n",
    "### 6.2. Le rapport (à remettre via CanvasLMS)  (15 points)\n",
    " \n",
    "Éditez un court rapport qui décrit **clairement** votre travail en incluant des résultats d'analyse comparative (benchmark) en termes de courbes ROC et ASC sur le **jeu de données Yosemite** fourni dans le répertoire `resources`. Pour calculer des courbes ROC\n",
    "\n",
    "1. Exécutez **`featuresUI.py`**. \n",
    "2. Puis, basculez vers l'onglet \"Benchmark\" et appuyez sur \"Run Benchmark\".\n",
    "3. Ensuite, sélectionnez le répertoire \"resources/yosemite\". \n",
    "\n",
    "Enfin, l'ASC sera affichée en bas de l'écran (AUC en anglais) et vous pouvez enregistrer la courbe ROC en appuyant sur \"Screenshot\". Vous devez également inclure une image du détecteur Harris. Elle est enregistrée sous le nom `harris.png` chaque fois que les points-clés Harris sont calculés.\n",
    "\n",
    "Le rapport doit contenir les éléments suivants :\n",
    "\n",
    "- Exécutez le benchmark dans **`featuresUI.py`** sur le jeu de données Yosemite pour les quatre configurations possibles impliquant des descripteurs simples ou MOPS avec SMC ou \"ratio de distances\". Incluez les courbes ROC résultantes, signalez les ASC obtenues, et indiquez quelle méthode produit les meilleurs résultats, d'après vous.\n",
    "\n",
    "\n",
    "- Inclure l'image Harris pour \"yosemite/yosemite1.jpg\". Commentez sur les caractéristiques des régions mises en évidence dans l'image (c.-à-d. les régions où il y a des points-clés détectés). Y a-t-il des régions de l'image qui auraient dû être mises en évidence mais ne le sont pas ?\n",
    "\n",
    "\n",
    "- Incluez une capture d'écran d'une paire de vos propres images et montrez visuellement les performances de mise en correspondance des primitives pour MOPS + \"Ratio de distances\".\n",
    "\n",
    "\n",
    "- Si vous avez réalisé la tache bonus, incluez les courbes ROC et indiquez les ASC de l'ensemble de données Yosemite avec votre algorithme personnalisé en utilisant le \"ratio de distances\". Décrivez vos modifications apportées à l'algorithme et pourquoi elles améliorent les performances.\n",
    "</del>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "vscode": {
   "interpreter": {
    "hash": "66abd9c73c372f28a8ef98b8462078e3c895e5bf5441957c690bca4abf6b5117"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
